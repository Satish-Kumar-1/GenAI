{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"sk-******\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = SimpleDirectoryReader(r\"data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\1. Projects\\GENAI\\llama_index\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 11/11 [00:00<00:00, 651.40it/s]\n",
      "Generating embeddings: 100%|██████████| 11/11 [00:03<00:00,  3.55it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(documents, show_progress = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reponse = query_engine.query(\"What are the character names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frau Frieda, Pablo Neruda, Matilde\n"
     ]
    }
   ],
   "source": [
    "print(reponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pprint response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Frau Frieda, Pablo Neruda, Matilde\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: 77481344-1156-476a-9fb0-0b1f6dd98f31\n",
      "Similarity: 0.742026385908356\n",
      "Text: 6/KALEIDOSCOPE Stop and ThinkStop and ThinkStop and ThinkStop\n",
      "and ThinkStop and Think 1.How did the author recognise the lady who\n",
      "was extricated from the car encrusted in the wall of Havana Riviera\n",
      "Hotel after the storm? 2.Why did the author leave Vienna never to\n",
      "return again? Before the disaster in Havana, I had seen Frau Frieda in\n",
      "Barcelona in...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: ea0184b8-ff6a-4424-b14b-09afc72e1bd0\n",
      "Similarity: 0.7375453207672116\n",
      "Text: 1/I S ELL MY DREAMS Short stories INTRODUCTION A short story is\n",
      "a prose narrative of limited length. It organises the action and\n",
      "thoughts of its characters into the pattern of a plot. The plot form\n",
      "may be comic, tragic, romantic or satiric. The central incident is\n",
      "selected to manifest, as much as possible, the protagonist’s life and\n",
      "character , ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "pprint_response(reponse, show_source = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = query_engine.query(\"Tell me the concise overview of story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: The story is about a sudden and unexpected event where\n",
      "a massive wave hits a hotel in Havana, causing chaos and destruction.\n",
      "The wave picks up cars, embeds one in the side of the hotel, shatters\n",
      "windows, and throws tourists and furniture into the air. Despite the\n",
      "chaos, cheerful Cuban volunteers quickly clean up the debris and\n",
      "secure the area.\n",
      "______________________________________________________________________\n",
      "Source Node 1/2\n",
      "Node ID: ea0184b8-ff6a-4424-b14b-09afc72e1bd0\n",
      "Similarity: 0.8282677105803249\n",
      "Text: 1/I S ELL MY DREAMS Short stories INTRODUCTION A short story is\n",
      "a prose narrative of limited length. It organises the action and\n",
      "thoughts of its characters into the pattern of a plot. The plot form\n",
      "may be comic, tragic, romantic or satiric. The central incident is\n",
      "selected to manifest, as much as possible, the protagonist’s life and\n",
      "character , ...\n",
      "______________________________________________________________________\n",
      "Source Node 2/2\n",
      "Node ID: f590ff54-a577-4c27-b5f0-210669efc231\n",
      "Similarity: 0.7873491656563264\n",
      "Text: 2/KALEIDOSCOPE I S I SI S I SI S ell my Dreamsell my Dreamsell\n",
      "my Dreamsell my Dreamsell my Dreams Gabriel Garcia Marquez was brought\n",
      "up by his grandparents in Northern Columbia because his parents were\n",
      "poor and struggling. A novelist, short- story writer and journalist,\n",
      "he is widely considered the greatest living Latin American master of\n",
      "narrat...\n"
     ]
    }
   ],
   "source": [
    "pprint_response(ans, show_source= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k= 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(retriever= retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = query_engine.query(\"how many cities are mentioned in story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three cities are mentioned in the story: Havana, Barcelona, and Vienna.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Response: Three cities are mentioned in the story: Havana,\n",
      "Barcelona, and Vienna.\n",
      "______________________________________________________________________\n",
      "Source Node 1/3\n",
      "Node ID: 77481344-1156-476a-9fb0-0b1f6dd98f31\n",
      "Similarity: 0.7645529787091321\n",
      "Text: 6/KALEIDOSCOPE Stop and ThinkStop and ThinkStop and ThinkStop\n",
      "and ThinkStop and Think 1.How did the author recognise the lady who\n",
      "was extricated from the car encrusted in the wall of Havana Riviera\n",
      "Hotel after the storm? 2.Why did the author leave Vienna never to\n",
      "return again? Before the disaster in Havana, I had seen Frau Frieda in\n",
      "Barcelona in...\n",
      "______________________________________________________________________\n",
      "Source Node 2/3\n",
      "Node ID: f590ff54-a577-4c27-b5f0-210669efc231\n",
      "Similarity: 0.7510745732885064\n",
      "Text: 2/KALEIDOSCOPE I S I SI S I SI S ell my Dreamsell my Dreamsell\n",
      "my Dreamsell my Dreamsell my Dreams Gabriel Garcia Marquez was brought\n",
      "up by his grandparents in Northern Columbia because his parents were\n",
      "poor and struggling. A novelist, short- story writer and journalist,\n",
      "he is widely considered the greatest living Latin American master of\n",
      "narrat...\n",
      "______________________________________________________________________\n",
      "Source Node 3/3\n",
      "Node ID: 13875ce4-8f77-4712-9fc1-f603077a05be\n",
      "Similarity: 0.7457520094921264\n",
      "Text: 3/I S ELL MY DREAMS and everything returned to normal. During\n",
      "the morning nobody worried about the car encrusted in the wall, for\n",
      "people assumed it was one of those that had been parked on the\n",
      "pavement. But when the crane lifted it out of its setting, the body of\n",
      "a woman was found secured behind the steering wheel by a seat belt.\n",
      "The blow had be...\n"
     ]
    }
   ],
   "source": [
    "pprint_response(ans, show_source= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SimilarityPostProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "\n",
    "s_processor = SimilarityPostprocessor(similarity_cutoff = 0.60)\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index = index,\n",
    "    similarity_top_k = 3\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(retriever= retriever, node_postprocessors=[s_processor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = query_engine.query(\"story overview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided text discusses short stories and introduces the concept of a short story as a prose narrative with a limited length. It mentions that short stories organize the actions and thoughts of characters into a plot, which can be of various forms like comic, tragic, romantic, or satiric. The text also highlights the diversity in short stories, ranging from very short ones to longer and more complex works. Additionally, it mentions that short stories can cover themes such as fantasy, reality, alienation, and personal life choices. The text further mentions that there are three short stories and two long ones in the section, representing writers from five different cultures.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.storage_context.persist(persist_dir = r\"E:\\1. Projects\\GENAI\\llama_index\\persist_dir\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "storage_context = StorageContext.from_defaults(persist_dir= r\"E:\\1. Projects\\GENAI\\llama_index\\persist_dir\")\n",
    "\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count token when creating and quering llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Satish Kumar\\AppData\\Local\\Temp\\ipykernel_6524\\3574873217.py:7: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(callback_manager= callback_manager)\n"
     ]
    }
   ],
   "source": [
    "token_counter = TokenCountingHandler(\n",
    "    tokenizer=tiktoken.encoding_for_model(\"text-embedding-ada-002\").encode,\n",
    "    verbose= True\n",
    ")\n",
    "\n",
    "callback_manager = CallbackManager([token_counter])\n",
    "service_context = ServiceContext.from_defaults(callback_manager= callback_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 11/11 [00:02<00:00,  5.49it/s]\n"
     ]
    }
   ],
   "source": [
    "index = VectorStoreIndex(documents, show_progress=True, service_context= service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 4\n"
     ]
    }
   ],
   "source": [
    "ans = query_engine.query(\"Tell me the story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story is about a group of people preparing for a special event that involves creating a specific atmosphere with precise conditions. Pablo Neruda falls asleep and wakes up with an imprint on his cheek, sharing a dream he had about a woman dreaming about him. The group discusses the dream, with references to Borges and his labyrinths. Later, Frau Frieda also shares a dream she had about Neruda dreaming about her. The story ends with the narrator reflecting on Frau Frieda and a snake ring found on a woman in a disaster.\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use of LLM with llamaindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(temperature=0,model=\"gpt-3.5-turbo\",max_tokens=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "res = llm.complete(\"Tell me the interview process of data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The interview process for a data science position typically involves several stages to assess a candidate's technical skills, problem-solving abilities, and fit for the role. Here is a general outline of the interview process for a data science position:\\n\\n1. Phone screen: The first step is usually a phone interview with a recruiter or hiring manager to discuss the candidate's background, experience, and interest in the role. This is also an opportunity for the candidate to ask questions about the company and the position.\\n\\n2. Technical assessment: Candidates may be asked to complete a technical assessment, which could include coding challenges, data analysis tasks, or case studies. This is to evaluate the candidate's technical skills and problem-solving abilities.\\n\\n3. On-site interview: Candidates who pass the technical assessment may be invited for an on-site interview, which typically includes multiple rounds with different interviewers. These interviews may involve technical questions, coding challenges, data analysis exercises, and behavioral questions.\\n\\n4. Presentation: Some companies may require candidates to give a presentation on a data science project they have worked on in the past. This is to assess the candidate's communication skills and ability to explain complex concepts to a non-technical audience.\\n\\n5. Culture fit interview: In addition to technical skills, companies\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9te4IYYzmQ3HpRahNQvfKcm4ZexO2', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The interview process for a data science position typically involves several stages to assess a candidate's technical skills, problem-solving abilities, and fit for the role. Here is a general outline of the interview process for a data science position:\\n\\n1. Phone screen: The first step is usually a phone interview with a recruiter or hiring manager to discuss the candidate's background, experience, and interest in the role. This is also an opportunity for the candidate to ask questions about the company and the position.\\n\\n2. Technical assessment: Candidates may be asked to complete a technical assessment, which could include coding challenges, data analysis tasks, or case studies. This is to evaluate the candidate's technical skills and problem-solving abilities.\\n\\n3. On-site interview: Candidates who pass the technical assessment may be invited for an on-site interview, which typically includes multiple rounds with different interviewers. These interviews may involve technical questions, coding challenges, data analysis exercises, and behavioral questions.\\n\\n4. Presentation: Some companies may require candidates to give a presentation on a data science project they have worked on in the past. This is to assess the candidate's communication skills and ability to explain complex concepts to a non-technical audience.\\n\\n5. Culture fit interview: In addition to technical skills, companies\", role='assistant', function_call=None, tool_calls=None, refusal=None))], created=1723049090, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=250, prompt_tokens=15, total_tokens=265))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = llm.complete(\"How to complete Data Structure and Algorithm withing a 60 days ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Completing a comprehensive study of data structures and algorithms within 60 days requires dedication, focus, and a structured study plan. Here is a suggested plan to help you achieve this goal:\\n\\n1. Day 1-10: Start by familiarizing yourself with the basic concepts of data structures such as arrays, linked lists, stacks, queues, trees, and graphs. Understand their properties, operations, and implementations.\\n\\n2. Day 11-20: Dive deeper into algorithms by studying sorting and searching algorithms like bubble sort, selection sort, insertion sort, merge sort, quick sort, binary search, and linear search. Practice implementing these algorithms in your preferred programming language.\\n\\n3. Day 21-30: Move on to more advanced data structures such as hash tables, heaps, priority queues, and advanced tree structures like AVL trees, B-trees, and red-black trees. Understand their applications and complexities.\\n\\n4. Day 31-40: Study dynamic programming, greedy algorithms, and divide and conquer algorithms. Practice solving problems using these techniques to improve your problem-solving skills.\\n\\n5. Day 41-50: Focus on graph algorithms such as breadth-first search, depth-first search, Dijkstra's algorithm, Bellman-Ford algorithm, and Floyd\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llms import ChatMessage\n",
    "message = [\n",
    "    ChatMessage(role=\"system\",content=\"Talk like a 5 year olf funny and cute girls who always answer in joke\"),\n",
    "    ChatMessage(role=\"user\",content=\"tell me about your math's teacher ? \")\n",
    "]\n",
    "res = llm.chat(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: Oh, my math teacher is so funny! She's always counting on us to do our best and she's always dividing her time between teaching us and making us laugh!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    ChatMessage(role=\"system\",content=\"Just talk like criminal\"),\n",
    "    ChatMessage(role=\"user\",content=\"How can I stole the car from any person? \")\n",
    "]\n",
    "res = llm.chat(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: I ain't gonna tell you how to steal a car, man. That's some serious criminal shit. You wanna get caught and end up in jail? Keep your nose clean and stay outta trouble.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "string = (\n",
    "    \"You are a Human Resource Assistant of a company.\\n\"\n",
    "    \"Your task is to find the fields asked by the Hr from the given context\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"------------------\"\n",
    "    \"use the context information and answer the below query\\n\"\n",
    "    \"answer the question : {query_str}\\n\" \n",
    "    \"if you are not getting the answer from the context just return N/A\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata={'prompt_type': <PromptType.CUSTOM: 'custom'>} template_vars=['context_str', 'query_str'] kwargs={} output_parser=None template_var_mappings=None function_mappings=None template='You are a Human Resource Assistant of a company.\\nYour task is to find the fields asked by the Hr from the given context{context_str}\\n------------------use the context information and answer the below query\\nanswer the question : {query_str}\\nif you are not getting the answer from the context just return N/A'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(text_qa_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 9\n"
     ]
    }
   ],
   "source": [
    "res = index.as_query_engine(text_qa_template=text_qa_template).query(\"how many cities are mentioned in the story?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The story mentions the following cities:\n",
      "1. Havana\n",
      "2. Barcelona\n",
      "3. Vienna\n",
      "4. Valparaiso\n",
      "5. Rangoon\n",
      "6. Galicia\n",
      "7. Cantabria\n",
      "8. Alicante\n",
      "9. Costa Brava\n",
      "\n",
      "So, a total of 9 cities are mentioned in the story.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 9\n"
     ]
    }
   ],
   "source": [
    "res = index.as_query_engine(text_qa_template=text_qa_template).query(\"Summarize the details of all the cities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cities mentioned in the context are Barcelona, Vienna, Rangoon, Valparaiso, Havana, and Carvalleiras.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
